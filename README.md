âœ‹ Hand Gesture Recognition Model

ğŸ“Œ Project Overview

This project implements a real-time hand gesture recognition system powered by a Convolutional Neural Network (CNN) trained on the LeapGestRecog Dataset
.

The trained model is integrated with OpenCV and PyCAW to control the system volume using hand gestures detected live via webcam.

ğŸ”„ Workflow

Image loading & preprocessing

CNN model training on gesture images

Model evaluation (accuracy, confusion matrix, classification report)

Real-time gesture detection with webcam

Mapping gestures â†’ system volume control (mute, unmute, increase, decrease)

ğŸ“Š Dataset

Source:
LeapGestRecog Dataset

Folder Structure:

LeapGestRecog/
  00/
  01/
  ...
  09/


Classes (subset used for control):

âœŠ Fist â†’ Mute ğŸ”‡

ğŸ– Palm â†’ Unmute ğŸ”Š

ğŸ‘ Thumbs Up â†’ Volume Up â¬†ï¸

ğŸ‘ Thumbs Down â†’ Volume Down â¬‡ï¸

Image Preprocessing:

Grayscale conversion

Resized to 64 Ã— 64

Normalized pixel values [0,1]

ğŸ§® Model Features

Architecture: CNN (3 Conv layers + BatchNorm + Dense layers)

Input size: 64 Ã— 64 grayscale

Output labels: 10 gesture classes (4 used for volume mapping)

Training:

Optimizer: Adam

Loss: Categorical Crossentropy

Epochs: 20

Batch size: 32

âš™ï¸ Requirements

Python 3.x

Libraries:

tensorflow

opencv-python

numpy

matplotlib

scikit-learn

mediapipe

pycaw

ğŸ“¥ Install all dependencies:

pip install -r requirements.txt

ğŸ”¬ Methodology

1ï¸âƒ£ Data Loading

Load images from LeapGestRecog dataset

Assign labels based on folder structure

2ï¸âƒ£ Preprocessing

Convert to grayscale

Resize to 64Ã—64

Normalize pixel values

3ï¸âƒ£ Model Training

Train CNN on gesture images

Save trained model as handGesture_model.keras

4ï¸âƒ£ Evaluation

Accuracy & loss curves

Confusion matrix

Classification report

5ï¸âƒ£ Real-Time Gesture Control

Capture webcam feed with OpenCV

Detect hand region using MediaPipe

Predict gesture inside ROI

Map gestures to volume actions

ğŸ“ˆ Results

Accuracy: ~98â€“99% on test set (10 classes)

Performance: Stable real-time gesture recognition (~15â€“20 FPS)

Volume Control: Smooth adjustment with natural gestures

âœ… Example:

Palm â†’ Unmute

Fist â†’ Mute

Thumbs Up â†’ Increase volume

Thumbs Down â†’ Decrease volume

ğŸ”® Future Improvements

Extend support to more gestures (Peace âœŒ, OK ğŸ‘Œ, etc.)

Improve robustness with better ROI extraction

Use transfer learning (MobileNetV2, EfficientNet) for higher accuracy

Deploy as a desktop app with PyQt or web app with Streamlit/Flask

